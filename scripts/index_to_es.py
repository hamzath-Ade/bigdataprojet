import os
import datetime
import logging
import time
import json
from pathlib import Path
from typing import Dict, List, Any, Optional

import pandas as pd
from elasticsearch import Elasticsearch, helpers
from elasticsearch.exceptions import (
    ConnectionError,
    RequestError,
    NotFoundError,
    ConflictError
)

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__)) + "/../"


def index_to_elasticsearch(**context):
    """
    Indexe les donn√©es combin√©es dans Elasticsearch avec gestion robuste d'erreurs

    Am√©liorations:
    - Gestion d'erreurs compl√®te (connexion, validation, indexation)
    - Configuration Elasticsearch optimis√©e
    - Validation du sch√©ma et des donn√©es
    - Logging d√©taill√© avec m√©triques
    - Indexation par batch avec retry
    - Monitoring de performance
    - Validation post-indexation
    - Gestion des conflits et doublons
    """
    today = datetime.date.today()
    today_str = today.isoformat()

    combined_path = Path(PROJECT_ROOT) / f"data/final/combined_{today_str}.parquet"

    logger.info(f"üîÑ D√©but de l'indexation Elasticsearch pour {today_str}")
    logger.info(f"üìÇ Fichier source: {combined_path}")

    # Validation du fichier d'entr√©e
    if not combined_path.exists():
        logger.error(f"‚ùå Fichier source introuvable: {combined_path}")
        raise FileNotFoundError(f"Fichier source introuvable: {combined_path}")

    if not any(combined_path.rglob('*.parquet')):
        logger.error(f"‚ùå Aucun fichier Parquet trouv√© dans: {combined_path}")
        raise ValueError(f"Aucun fichier Parquet trouv√© dans: {combined_path}")

    # Calcul de la taille du fichier
    total_size = sum(p.stat().st_size for p in combined_path.rglob('*') if p.is_file())
    logger.info(f"üìè Taille du fichier source: {total_size} bytes")

    # Configuration Elasticsearch
    es_config = {
        "hosts": ["http://localhost:9200"],
        "request_timeout": 30,
        #"max_retries": 3,
        "retry_on_timeout": True,
        "verify_certs": False,
        "http_compress": True
    }

    index_name = "air_asthma"

    # Mapping Elasticsearch optimis√©
    index_mapping = {
        "settings": {
            "number_of_shards": 1,
            "number_of_replicas": 0,  # Pas de r√©plication en d√©veloppement
            "refresh_interval": "30s",
            "analysis": {
                "analyzer": {
                    "default": {
                        "type": "standard"
                    }
                }
            }
        },
        "mappings": {
            "properties": {
                "year": {
                    "type": "integer",
                    "index": True
                },
                "pm25_avg_yearly": {
                    "type": "float",
                    "index": True
                },
                "pm25_min_yearly": {
                    "type": "float",
                    "index": True
                },
                "pm25_max_yearly": {
                    "type": "float",
                    "index": True
                },
                "pm25_measurements_count": {
                    "type": "integer",
                    "index": True
                },
                "daily_measurements_avg": {
                    "type": "float",
                    "index": True
                },
                "percentage": {
                    "type": "text",
                    "index": False  # Texte, pas besoin d'indexation
                },
                "group": {
                    "type": "keyword",
                    "index": True
                },
                "outcome_or_indicator": {
                    "type": "keyword",
                    "index": True
                },
                "confidence_interval": {
                    "type": "text",
                    "index": False
                },
                "indexed_at": {
                    "type": "date",
                    "format": "strict_date_optional_time||epoch_millis"
                },
                "data_source": {
                    "type": "keyword",
                    "index": True
                },
                "processing_date": {
                    "type": "date",
                    "format": "strict_date_optional_time||epoch_millis"
                }
            }
        }
    }

    es = None
    try:
        # Initialisation du client Elasticsearch
        logger.info("üîå Connexion √† Elasticsearch...")
        es = Elasticsearch(**es_config)

        # Test de connexion
        if not es.ping():
            logger.error("‚ùå Impossible de se connecter √† Elasticsearch")
            raise ConnectionError("Impossible de se connecter √† Elasticsearch")

        # Informations sur le cluster
        cluster_info = es.info()
        logger.info(f"‚úÖ Connexion r√©ussie - Version ES: {cluster_info['version']['number']}")
        logger.info(f"üè∑Ô∏è Cluster: {cluster_info['cluster_name']}")

        # V√©rification de la sant√© du cluster
        health = es.cluster.health()
        logger.info(f"üè• Sant√© du cluster: {health['status']}")

        if health['status'] == 'red':
            logger.warning("‚ö†Ô∏è Cluster en √©tat RED - Continuons avec prudence")

        # Lecture du fichier Parquet
        logger.info("üìñ Lecture du fichier Parquet...")
        start_read = time.time()

        try:
            df = pd.read_parquet(combined_path)
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la lecture du fichier Parquet: {e}")
            raise

        read_time = time.time() - start_read
        logger.info(f"‚úÖ Fichier lu en {read_time:.2f}s")

        initial_count = len(df)
        logger.info(f"üìä Nombre d'enregistrements lus: {initial_count}")

        if initial_count == 0:
            logger.error("‚ùå Aucun enregistrement trouv√© dans le fichier")
            raise ValueError("Aucun enregistrement trouv√© dans le fichier")

        # Affichage du sch√©ma des donn√©es
        logger.info("üîç Sch√©ma des donn√©es:")
        logger.info(f"Colonnes: {list(df.columns)}")
        logger.info(f"Types: {df.dtypes.to_dict()}")

        # Affichage d'un √©chantillon
        logger.info("üîç √âchantillon des donn√©es:")
        logger.info(df.head(3).to_string())

        # Validation et nettoyage des donn√©es
        logger.info("üßπ Validation et nettoyage des donn√©es...")

        # V√©rification des valeurs nulles
        null_counts = df.isnull().sum()
        null_columns = null_counts[null_counts > 0]

        if len(null_columns) > 0:
            logger.warning("‚ö†Ô∏è Colonnes avec valeurs nulles:")
            for col, count in null_columns.items():
                logger.warning(f"   {col}: {count} valeurs nulles")

        # Nettoyage des donn√©es
        df_clean = df.copy()

        # Conversion des types pour √©viter les erreurs
        numeric_columns = ['pm25_avg_yearly', 'pm25_min_yearly', 'pm25_max_yearly',
                           'pm25_measurements_count', 'daily_measurements_avg']

        for col in numeric_columns:
            if col in df_clean.columns:
                df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')

        # Suppression des lignes avec des valeurs critiques manquantes
        critical_columns = ['year']
        df_clean = df_clean.dropna(subset=critical_columns)

        clean_count = len(df_clean)
        logger.info(f"üìä Enregistrements apr√®s nettoyage: {clean_count}")

        if clean_count == 0:
            logger.error("‚ùå Aucun enregistrement valide apr√®s nettoyage")
            raise ValueError("Aucun enregistrement valide apr√®s nettoyage")

        logger.info(f"üìâ Enregistrements supprim√©s: {initial_count - clean_count}")

        # Gestion de l'index Elasticsearch
        logger.info(f"üèóÔ∏è Gestion de l'index '{index_name}'...")

        index_exists = es.indices.exists(index=index_name)

        if index_exists:
            logger.info(f"üìã Index '{index_name}' existe d√©j√†")

            # Obtenir les statistiques de l'index existant
            try:
                stats = es.indices.stats(index=index_name)
                existing_docs = stats['indices'][index_name]['total']['docs']['count']
                logger.info(f"üìä Documents existants dans l'index: {existing_docs}")

                # Demander confirmation pour la r√©indexation
                logger.info("üîÑ Suppression et recr√©ation de l'index...")
                es.indices.delete(index=index_name)
                logger.info(f"‚úÖ Index '{index_name}' supprim√©")

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur lors de la r√©cup√©ration des stats: {e}")

        # Cr√©ation de l'index avec le mapping optimis√©
        logger.info(f"üèóÔ∏è Cr√©ation de l'index '{index_name}'...")

        try:
            es.indices.create(index=index_name, body=index_mapping)
            logger.info(f"‚úÖ Index '{index_name}' cr√©√© avec succ√®s")
        except RequestError as e:
            if "already_exists_exception" in str(e):
                logger.info(f"üìã Index '{index_name}' existe d√©j√†")
            else:
                logger.error(f"‚ùå Erreur lors de la cr√©ation de l'index: {e}")
                raise

        # Pr√©paration des documents pour l'indexation
        logger.info("üìù Pr√©paration des documents pour l'indexation...")

        actions = []
        error_count = 0
        indexed_at = datetime.datetime.now().isoformat()

        for index, row in df_clean.iterrows():
            try:
                # Construction du document
                doc_source = {}

                # Ajout de toutes les colonnes disponibles
                for col in df_clean.columns:
                    value = row[col]

                    # Gestion des valeurs NaN/None
                    if pd.isna(value):
                        doc_source[col] = None
                    elif isinstance(value, (int, float)):
                        # Conversion des valeurs num√©riques
                        if pd.isna(value) or value != value:  # Check for NaN
                            doc_source[col] = None
                        else:
                            doc_source[col] = float(value) if isinstance(value, float) else int(value)
                    else:
                        doc_source[col] = str(value)

                # Ajout de m√©tadonn√©es
                doc_source.update({
                    "indexed_at": indexed_at,
                    "data_source": f"combined_{today_str}",
                    "processing_date": today_str
                })

                # Construction de l'action d'indexation
                action = {
                    "_index": index_name,
                    "_id": f"{today_str}_{index}",  # ID unique bas√© sur la date et l'index
                    "_source": doc_source
                }

                actions.append(action)

            except Exception as e:
                error_count += 1
                logger.warning(f"‚ö†Ô∏è Erreur lors de la pr√©paration du document {index}: {e}")
                continue

        docs_prepared = len(actions)
        logger.info(f"üìù Documents pr√©par√©s: {docs_prepared}")

        if error_count > 0:
            logger.warning(f"‚ö†Ô∏è Erreurs de pr√©paration: {error_count}")

        if docs_prepared == 0:
            logger.error("‚ùå Aucun document pr√©par√© pour l'indexation")
            raise ValueError("Aucun document pr√©par√© pour l'indexation")

        # Indexation par batch avec gestion d'erreurs
        logger.info("üöÄ D√©but de l'indexation...")

        batch_size = 100
        success_count = 0
        failed_count = 0
        start_index = time.time()

        try:
            # Configuration pour l'indexation bulk
            bulk_config = {
                'chunk_size': batch_size,
                'max_chunk_bytes': 10 * 1024 * 1024,  # 10MB
                'request_timeout': 60,
                #'max_retries': 3,
                #'initial_backoff': 2,
                #'max_backoff': 600
            }

            # Indexation avec helpers.bulk
            for success, info in helpers.parallel_bulk(
                    es,
                    actions,
                    **bulk_config
            ):
                if success:
                    success_count += 1
                else:
                    failed_count += 1
                    logger.warning(f"‚ö†Ô∏è √âchec d'indexation: {info}")

                # Log de progression
                if (success_count + failed_count) % 50 == 0:
                    logger.info(f"üìä Progression: {success_count + failed_count}/{docs_prepared} documents")

        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'indexation bulk: {e}")
            raise

        index_time = time.time() - start_index

        logger.info(f"‚úÖ Indexation termin√©e en {index_time:.2f}s")
        logger.info(f"üìä Documents index√©s avec succ√®s: {success_count}")

        if failed_count > 0:
            logger.warning(f"‚ö†Ô∏è Documents √©chou√©s: {failed_count}")

        # Actualisation de l'index pour les recherches imm√©diates
        logger.info("üîÑ Actualisation de l'index...")
        es.indices.refresh(index=index_name)

        # Validation post-indexation
        logger.info("üîç Validation post-indexation...")

        # Attendre un peu pour que l'indexation soit compl√®tement termin√©e
        time.sleep(2)

        # V√©rification du nombre de documents index√©s
        doc_count = es.count(index=index_name)['count']
        logger.info(f"üìä Documents dans l'index apr√®s indexation: {doc_count}")

        if doc_count == success_count:
            logger.info("‚úÖ Validation r√©ussie - Tous les documents sont index√©s")
        else:
            logger.warning(f"‚ö†Ô∏è Incoh√©rence d√©tect√©e - Attendu: {success_count}, Trouv√©: {doc_count}")

        # Test de recherche
        logger.info("üîç Test de recherche...")
        try:
            search_result = es.search(
                index=index_name,
                body={
                    "query": {"match_all": {}},
                    "size": 1,
                    "sort": [{"year": {"order": "desc"}}]
                }
            )

            if search_result['hits']['total']['value'] > 0:
                sample_doc = search_result['hits']['hits'][0]['_source']
                logger.info("‚úÖ Test de recherche r√©ussi")
                logger.info(f"üîç Exemple de document index√©: {json.dumps(sample_doc, indent=2, default=str)}")
            else:
                logger.warning("‚ö†Ô∏è Aucun document trouv√© lors du test de recherche")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur lors du test de recherche: {e}")

        # Statistiques finales
        logger.info("üìä Statistiques d'indexation:")
        logger.info(f"   ‚Ä¢ Fichier source: {combined_path}")
        logger.info(f"   ‚Ä¢ Taille fichier: {total_size} bytes")
        logger.info(f"   ‚Ä¢ Enregistrements lus: {initial_count}")
        logger.info(f"   ‚Ä¢ Enregistrements nettoy√©s: {clean_count}")
        logger.info(f"   ‚Ä¢ Documents pr√©par√©s: {docs_prepared}")
        logger.info(f"   ‚Ä¢ Documents index√©s: {success_count}")
        logger.info(f"   ‚Ä¢ Documents √©chou√©s: {failed_count}")
        logger.info(f"   ‚Ä¢ Temps de lecture: {read_time:.2f}s")
        logger.info(f"   ‚Ä¢ Temps d'indexation: {index_time:.2f}s")
        logger.info(f"   ‚Ä¢ Index Elasticsearch: {index_name}")

        logger.info("üéâ Indexation Elasticsearch termin√©e avec succ√®s!")

    except ConnectionError as e:
        logger.error(f"‚ùå Erreur de connexion Elasticsearch: {e}")
        raise
    except RequestError as e:
        logger.error(f"‚ùå Erreur de requ√™te Elasticsearch: {e}")
        raise
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'indexation: {type(e).__name__}: {e}")
        raise

    finally:
        # Pas de nettoyage sp√©cifique n√©cessaire pour le client ES
        if es:
            logger.info("‚úÖ Connexion Elasticsearch ferm√©e")


def validate_elasticsearch_index(index_name: str = "air_asthma", date_str: Optional[str] = None):
    """
    Fonction de validation pour v√©rifier l'index Elasticsearch
    """
    if date_str is None:
        date_str = datetime.date.today().isoformat()

    logger.info(f"üîç Validation de l'index Elasticsearch '{index_name}' pour {date_str}")

    try:
        es = Elasticsearch(hosts=["http://localhost:9200"], request_timeout=30)

        if not es.ping():
            logger.error("‚ùå Impossible de se connecter √† Elasticsearch")
            return False

        # V√©rification de l'existence de l'index
        if not es.indices.exists(index=index_name):
            logger.error(f"‚ùå Index '{index_name}' introuvable")
            return False

        # Statistiques de l'index
        stats = es.indices.stats(index=index_name)
        doc_count = stats['indices'][index_name]['total']['docs']['count']
        size_bytes = stats['indices'][index_name]['total']['store']['size_in_bytes']

        logger.info(f"‚úÖ Index valide avec {doc_count} documents ({size_bytes} bytes)")

        # Test de recherche par ann√©e
        search_result = es.search(
            index=index_name,
            body={
                "query": {"match_all": {}},
                "aggs": {
                    "years": {
                        "terms": {"field": "year", "size": 10}
                    },
                    "avg_pm25": {
                        "avg": {"field": "pm25_avg_yearly"}
                    }
                },
                "size": 0
            }
        )

        # Affichage des agr√©gations
        if 'aggregations' in search_result:
            years = [bucket['key'] for bucket in search_result['aggregations']['years']['buckets']]
            avg_pm25 = search_result['aggregations']['avg_pm25']['value']

            logger.info(f"üìÖ Ann√©es disponibles: {sorted(years)}")
            logger.info(f"üå¨Ô∏è PM2.5 moyen global: {avg_pm25:.2f} ¬µg/m¬≥")

        return True

    except Exception as e:
        logger.error(f"‚ùå Erreur de validation: {e}")
        return False


def search_elasticsearch_data(
        index_name: str = "air_asthma",
        query: Optional[Dict[str, Any]] = None,
        size: int = 10
) -> List[Dict[str, Any]]:
    """
    Fonction utilitaire pour rechercher dans l'index Elasticsearch
    """
    logger.info(f"üîç Recherche dans l'index '{index_name}'")

    try:
        es = Elasticsearch(hosts=["http://localhost:9200"], request_timeout=30)

        if not es.ping():
            logger.error("‚ùå Impossible de se connecter √† Elasticsearch")
            return []

        # Requ√™te par d√©faut si aucune fournie
        if query is None:
            query = {"match_all": {}}

        search_body = {
            "query": query,
            "size": size,
            "sort": [{"year": {"order": "desc"}}]
        }

        result = es.search(index=index_name, body=search_body)

        documents = [hit['_source'] for hit in result['hits']['hits']]
        total = result['hits']['total']['value']

        logger.info(f"‚úÖ {len(documents)} documents trouv√©s sur {total} total")

        return documents

    except Exception as e:
        logger.error(f"‚ùå Erreur de recherche: {e}")
        return []


if __name__ == "__main__":
    # Test en standalone
    logger.info("üöÄ Ex√©cution en mode test - Indexation Elasticsearch")
    logger.info("=" * 70)

    try:
        index_to_elasticsearch()
        logger.info("=" * 70)
        validate_elasticsearch_index()

        # Test de recherche
        logger.info("=" * 70)
        logger.info("üîç Test de recherche:")
        results = search_elasticsearch_data(size=3)
        for i, doc in enumerate(results, 1):
            logger.info(f"Document {i}: Year {doc.get('year')}, PM2.5: {doc.get('pm25_avg_yearly')}")

    except Exception as e:
        logger.error(f"‚ùå √âchec du test: {e}")
        exit(1)