import os
import datetime
import logging
import json
from pathlib import Path

from pyspark.shell import spark
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, isnan, isnull, regexp_replace, trim
from pyspark.sql.types import DoubleType, IntegerType

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
#spark.sparkContext._jsc.hadoopConfiguration().set("hadoop.native.io", "false")


PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__)) + "/../"
#PROJECT_ROOT = "D:/MonProjetBigData"

def transform_hospitalizations(**context):
    """
    Transforme les donnÃ©es brutes d'hospitalisations CDC au format Parquet

    AmÃ©liorations:
    - Gestion d'erreurs robuste
    - Validation des donnÃ©es d'entrÃ©e et de sortie
    - Nettoyage et standardisation des donnÃ©es
    - Logging dÃ©taillÃ©
    - Gestion des valeurs manquantes
    - Optimisation Spark
    """
    year = datetime.date.today().year
    raw_path = Path(PROJECT_ROOT) / f"data/raw/hospitalizations/{year}/asthma_rate.json"
    formatted_dir = Path(PROJECT_ROOT) / "data/formatted/hospitalizations"
    formatted_dir.mkdir(parents=True, exist_ok=True)

    out_path = formatted_dir / f"asthma_{year}.parquet"

    logger.info(f"ğŸ”„ DÃ©but de la transformation des donnÃ©es d'hospitalisation pour {year}")
    logger.info(f"ğŸ“‚ Fichier source: {raw_path}")
    logger.info(f"ğŸ“‚ Dossier destination: {formatted_dir}")

    # Validation du fichier d'entrÃ©e
    if not raw_path.exists():
        logger.error(f"âŒ Fichier source introuvable: {raw_path}")
        raise FileNotFoundError(f"Fichier source introuvable: {raw_path}")

    if raw_path.stat().st_size == 0:
        logger.error(f"âŒ Fichier source vide: {raw_path}")
        raise ValueError(f"Fichier source vide: {raw_path}")

    logger.info(f"ğŸ“ Taille du fichier source: {raw_path.stat().st_size} bytes")

    # Validation du contenu JSON
    try:
        with open(raw_path, "r", encoding="utf-8") as f:
            raw_data = json.load(f)

        if not raw_data:
            logger.error("âŒ Fichier JSON vide ou invalide")
            raise ValueError("Fichier JSON vide ou invalide")

        if not isinstance(raw_data, list):
            logger.error(f"âŒ Format JSON inattendu: attendu list, reÃ§u {type(raw_data)}")
            raise ValueError(f"Format JSON inattendu: attendu list, reÃ§u {type(raw_data)}")

        logger.info(f"âœ… Fichier JSON valide avec {len(raw_data)} enregistrements")

    except json.JSONDecodeError as e:
        logger.error(f"âŒ Erreur de parsing JSON: {e}")
        raise
    except Exception as e:
        logger.error(f"âŒ Erreur lors de la validation du fichier: {e}")
        raise

    # Configuration Spark optimisÃ©e
    spark_config = {
        "spark.app.name": "TransformHospitalizations",
        "spark.sql.adaptive.enabled": "true",
        "spark.sql.adaptive.coalescePartitions.enabled": "true",
        "spark.sql.execution.arrow.pyspark.enabled": "true",
        "spark.sql.repl.eagerEval.enabled": "true",
        "spark.sql.repl.eagerEval.maxNumRows": 20
    }

    spark = None
    try:
        logger.info("ğŸš€ Initialisation de Spark Session...")

        # Construction de la session Spark avec configuration
        builder = SparkSession.builder
        for key, value in spark_config.items():
            builder = builder.config(key, value)

        #spark = builder.getOrCreate()
        #spark.sparkContext.setLogLevel("WARN")  # RÃ©duire les logs Spark
        spark = builder.getOrCreate()
        spark.sparkContext.setLogLevel("WARN")  # RÃ©duire les logs Spark
        spark.sparkContext._jsc.hadoopConfiguration().set("hadoop.native.io", "false")

        logger.info(f"âœ… Spark Session crÃ©Ã©e - Version: {spark.version}")

        # Lecture du fichier JSON
        logger.info("ğŸ“– Lecture du fichier JSON...")
        df = spark.read.option("multiline", "true").json(str(raw_path))

        initial_count = df.count()
        logger.info(f"ğŸ“Š Nombre d'enregistrements lus: {initial_count}")

        if initial_count == 0:
            logger.error("âŒ Aucun enregistrement trouvÃ© dans le DataFrame")
            raise ValueError("Aucun enregistrement trouvÃ© dans le DataFrame")

        # Affichage du schÃ©ma pour debug
        logger.info("ğŸ” SchÃ©ma des donnÃ©es:")
        df.printSchema()

        # Affichage d'exemples d'enregistrements
        logger.info("ğŸ” Exemples d'enregistrements:")
        df.show(5, truncate=False)

        # VÃ©rification des colonnes nÃ©cessaires
        required_columns = ['year', 'percentage']
        available_columns = df.columns
        missing_columns = [col for col in required_columns if col not in available_columns]

        if missing_columns:
            logger.error(f"âŒ Colonnes manquantes: {missing_columns}")
            logger.info(f"ğŸ” Colonnes disponibles: {available_columns}")
            raise ValueError(f"Colonnes manquantes: {missing_columns}")

        logger.info(f"âœ… Toutes les colonnes requises sont prÃ©sentes")

        # Nettoyage et transformation des donnÃ©es
        logger.info("ğŸ§¹ Nettoyage et transformation des donnÃ©es...")

        # SÃ©lection et renommage des colonnes
        df_clean = df.select(
            col("year").cast(IntegerType()).alias("year"),
            col("confidence_interval").alias("confidence_interval"),
            col("description").alias("description"),
            col("group").alias("group"),
            #col("unit").alias("unit"),
            col("percentage").alias("percentage"),
            col("outcome_or_indicator").alias("outcome_or_indicator")
        )

        # Nettoyage de la colonne value
        df_clean = df_clean.withColumn(
            "asthma_rate",
            when(
                col("percentage").rlike("^[0-9]+\\.?[0-9]*$"),  # Regex pour nombres valides
                col("percentage").cast(DoubleType())
            ).otherwise(None)
        )

        # Nettoyage des chaÃ®nes de caractÃ¨res
        string_columns = ["group", "outcome_or_indicator", "percentage"]
        for column in string_columns:
            if column in df_clean.columns:
                df_clean = df_clean.withColumn(
                    column,
                    trim(regexp_replace(col(column), "[\\r\\n\\t]", " "))
                )

        # Filtrage des enregistrements valides
        df_final = df_clean.filter(
            col("year").isNotNull() &
            col("asthma_rate").isNotNull() &
            (col("year") >= 2000) &  # AnnÃ©es rÃ©alistes
            (col("asthma_rate") >= 0)  # Taux positifs
        ).select("year", "percentage", "group", "outcome_or_indicator", "confidence_interval")

        # VÃ©rification aprÃ¨s nettoyage
        final_count = df_final.count()
        logger.info(f"ğŸ“Š Enregistrements aprÃ¨s nettoyage: {final_count}")

        if final_count == 0:
            logger.error("âŒ Aucun enregistrement valide aprÃ¨s nettoyage")
            raise ValueError("Aucun enregistrement valide aprÃ¨s nettoyage")

        logger.info(f"ğŸ“‰ Enregistrements supprimÃ©s: {initial_count - final_count}")

        # Affichage des statistiques
        logger.info("ğŸ“ˆ Statistiques des donnÃ©es nettoyÃ©es:")
        df_final.describe().show()

        # Sauvegarde en Parquet
        logger.info(f"ğŸ’¾ Sauvegarde en cours vers: {out_path}")

        df_final.coalesce(1).write.mode("overwrite").parquet(str(out_path))

        logger.info(f"âœ… DonnÃ©es sauvegardÃ©es avec succÃ¨s dans: {out_path}")

        # Validation du fichier de sortie
        if out_path.exists():
            # Calcul de la taille du dossier Parquet
            total_size = sum(p.stat().st_size for p in out_path.rglob('*') if p.is_file())
            logger.info(f"ğŸ“ Taille du fichier Parquet: {total_size} bytes")

            # Test de relecture pour validation
            logger.info("ğŸ” Validation du fichier de sortie...")
            df_test = spark.read.parquet(str(out_path))
            test_count = df_test.count()

            if test_count == final_count:
                logger.info(f"âœ… Validation rÃ©ussie: {test_count} enregistrements")

                # Affichage d'un Ã©chantillon des donnÃ©es finales
                logger.info("ğŸ” Ã‰chantillon des donnÃ©es finales:")
                df_test.show(5)

            else:
                logger.error(f"âŒ Erreur de validation: attendu {final_count}, trouvÃ© {test_count}")
                raise ValueError("Erreur lors de la validation du fichier de sortie")
        else:
            logger.error("âŒ Le fichier de sortie n'a pas Ã©tÃ© crÃ©Ã©")
            raise ValueError("Le fichier de sortie n'a pas Ã©tÃ© crÃ©Ã©")

        logger.info("ğŸ‰ Transformation terminÃ©e avec succÃ¨s!")

    except Exception as e:
        logger.error(f"âŒ Erreur lors de la transformation: {type(e).__name__}: {e}")
        raise

    finally:
        # Nettoyage des ressources Spark
        if spark:
            logger.info("ğŸ§¹ Fermeture de la session Spark...")
            spark.stop()
            logger.info("âœ… Session Spark fermÃ©e")


def validate_transformed_data(year=None):
    """
    Fonction de validation pour vÃ©rifier les donnÃ©es transformÃ©es
    """
    if year is None:
        year = datetime.date.today().year

    formatted_path = Path(PROJECT_ROOT) / "data/formatted/hospitalizations" / f"asthma_{year}.parquet"

    logger.info(f"ğŸ” Validation des donnÃ©es transformÃ©es pour {year}")
    logger.info(f"ğŸ“‚ Fichier: {formatted_path}")

    if not formatted_path.exists():
        logger.error(f"âŒ Fichier transformÃ© introuvable: {formatted_path}")
        return False

    try:
        spark = SparkSession.builder.appName("ValidateTransformedData").getOrCreate()
        spark.sparkContext.setLogLevel("WARN")

        df = spark.read.parquet(str(formatted_path))
        count = df.count()

        logger.info(f"âœ… Fichier valide avec {count} enregistrements")

        # Statistiques rapides
        logger.info("ğŸ“Š Statistiques:")
        df.agg(
            {"percentage": "min", "percentage": "max", "percentage": "avg"}
        ).show()

        spark.stop()
        return True

    except Exception as e:
        logger.error(f"âŒ Erreur de validation: {e}")
        return False


if __name__ == "__main__":
    # Test en standalone
    logger.info("ğŸš€ ExÃ©cution en mode test")
    logger.info("=" * 50)

    try:
        transform_hospitalizations()
        logger.info("=" * 50)
        validate_transformed_data()

    except Exception as e:
        logger.error(f"âŒ Ã‰chec du test: {e}")
        exit(1)