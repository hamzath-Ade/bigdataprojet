import os
import datetime
import logging
from pathlib import Path

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    year as spark_year, col, when, isnan, isnull,
    count as spark_count, avg as spark_avg,
    min as spark_min, max as spark_max,
    sum as spark_sum, round as spark_round
)
from pyspark.sql.types import IntegerType

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__)) + "/../"


def combine_datasets(**context):
    """
    Combine les donnÃ©es de qualitÃ© de l'air et d'hospitalisation pour asthme

    AmÃ©liorations:
    - Gestion d'erreurs robuste
    - Validation des fichiers d'entrÃ©e et de sortie
    - Logging dÃ©taillÃ© avec mÃ©triques
    - Optimisation Spark
    - Gestion des valeurs manquantes
    - Calcul de statistiques de corrÃ©lation
    - Validation de la cohÃ©rence des donnÃ©es
    """
    today = datetime.date.today()
    today_str = today.isoformat()
    year = today.year

    # Calcul de la date d'hier pour les donnÃ©es de qualitÃ© de l'air
    yesterday = today - datetime.timedelta(days=1)
    yesterday_str = yesterday.isoformat()

    # Chemins des fichiers
    aq_path = Path(PROJECT_ROOT) / f"data/formatted/air_quality/air_quality_{yesterday_str}.parquet"
    hosp_path = Path(PROJECT_ROOT) / f"data/formatted/hospitalizations/asthma_{year}.parquet"
    final_dir = Path(PROJECT_ROOT) / "data/final"
    final_dir.mkdir(parents=True, exist_ok=True)

    out_path = final_dir / f"combined_{today_str}.parquet"

    logger.info(f"ğŸ”„ DÃ©but de la combinaison des datasets pour {today_str}")
    logger.info(f"ğŸ“‚ QualitÃ© de l'air: {aq_path}")
    logger.info(f"ğŸ“‚ Hospitalisations: {hosp_path}")
    logger.info(f"ğŸ“‚ Sortie finale: {out_path}")

    # Validation des fichiers d'entrÃ©e
    missing_files = []
    file_sizes = {}

    if not aq_path.exists():
        missing_files.append(f"QualitÃ© de l'air: {aq_path}")
    else:
        file_sizes['air_quality'] = sum(p.stat().st_size for p in aq_path.rglob('*') if p.is_file())
        logger.info(f"ğŸ“ Taille fichier qualitÃ© de l'air: {file_sizes['air_quality']} bytes")

    if not hosp_path.exists():
        missing_files.append(f"Hospitalisations: {hosp_path}")
    else:
        file_sizes['hospitalizations'] = sum(p.stat().st_size for p in hosp_path.rglob('*') if p.is_file())
        logger.info(f"ğŸ“ Taille fichier hospitalisations: {file_sizes['hospitalizations']} bytes")

    if missing_files:
        logger.error(f"âŒ Fichiers source manquants:")
        for file in missing_files:
            logger.error(f"   - {file}")
        raise FileNotFoundError(f"Fichiers source manquants: {missing_files}")

    # VÃ©rification des tailles de fichier
    for name, size in file_sizes.items():
        if size == 0:
            logger.error(f"âŒ Fichier {name} vide")
            raise ValueError(f"Fichier {name} vide")

    logger.info("âœ… Tous les fichiers source sont prÃ©sents et non vides")

    # Configuration Spark optimisÃ©e
    spark_config = {
        "spark.app.name": "CombineDatasets",
        "spark.sql.adaptive.enabled": "true",
        "spark.sql.adaptive.coalescePartitions.enabled": "true",
        "spark.sql.execution.arrow.pyspark.enabled": "true",
        "spark.sql.repl.eagerEval.enabled": "true",
        "spark.sql.repl.eagerEval.maxNumRows": 20,
        "spark.serializer": "org.apache.spark.serializer.KryoSerializer",
        "spark.sql.adaptive.skewJoin.enabled": "true"
    }

    spark = None
    try:
        logger.info("ğŸš€ Initialisation de Spark Session...")

        # Construction de la session Spark avec configuration
        builder = SparkSession.builder
        for key, value in spark_config.items():
            builder = builder.config(key, value)

        spark = builder.getOrCreate()
        spark.sparkContext.setLogLevel("WARN")  # RÃ©duire les logs Spark
        spark.sparkContext._jsc.hadoopConfiguration().set("hadoop.native.io", "false")

        logger.info(f"âœ… Spark Session crÃ©Ã©e - Version: {spark.version}")

        # Lecture des donnÃ©es de qualitÃ© de l'air
        logger.info("ğŸ“– Lecture des donnÃ©es de qualitÃ© de l'air...")
        df_aq = spark.read.parquet(str(aq_path))

        aq_count = df_aq.count()
        logger.info(f"ğŸ“Š Enregistrements qualitÃ© de l'air: {aq_count}")

        if aq_count == 0:
            logger.error("âŒ Aucun enregistrement dans les donnÃ©es de qualitÃ© de l'air")
            raise ValueError("Aucun enregistrement dans les donnÃ©es de qualitÃ© de l'air")

        # Affichage du schÃ©ma et Ã©chantillon des donnÃ©es AQ
        logger.info("ğŸ” SchÃ©ma des donnÃ©es de qualitÃ© de l'air:")
        df_aq.printSchema()

        logger.info("ğŸ” Ã‰chantillon des donnÃ©es de qualitÃ© de l'air:")
        df_aq.show(5, truncate=False)

        # Lecture des donnÃ©es d'hospitalisation
        logger.info("ğŸ“– Lecture des donnÃ©es d'hospitalisation...")
        df_hosp = spark.read.parquet(str(hosp_path))

        hosp_count = df_hosp.count()
        logger.info(f"ğŸ“Š Enregistrements hospitalisation: {hosp_count}")

        if hosp_count == 0:
            logger.error("âŒ Aucun enregistrement dans les donnÃ©es d'hospitalisation")
            raise ValueError("Aucun enregistrement dans les donnÃ©es d'hospitalisation")

        # Affichage du schÃ©ma et Ã©chantillon des donnÃ©es hospitalisation
        logger.info("ğŸ” SchÃ©ma des donnÃ©es d'hospitalisation:")
        df_hosp.printSchema()

        logger.info("ğŸ” Ã‰chantillon des donnÃ©es d'hospitalisation:")
        df_hosp.show(5, truncate=False)

        # PrÃ©paration des donnÃ©es de qualitÃ© de l'air
        logger.info("ğŸ”„ PrÃ©paration des donnÃ©es de qualitÃ© de l'air...")

        # Ajout de la colonne annÃ©e et nettoyage
        df_aq_prepared = df_aq.withColumn("year", spark_year(col("date")).cast(IntegerType()))

        # VÃ©rification des annÃ©es prÃ©sentes
        years_aq = df_aq_prepared.select("year").distinct().collect()
        years_aq_list = [row['year'] for row in years_aq if row['year'] is not None]
        logger.info(f"ğŸ“… AnnÃ©es prÃ©sentes dans les donnÃ©es AQ: {sorted(years_aq_list)}")

        # Filtrage des donnÃ©es valides
        df_aq_clean = df_aq_prepared.filter(
            col("year").isNotNull() &
            col("pm25").isNotNull() &
            (col("pm25") >= 0)
        )

        aq_clean_count = df_aq_clean.count()
        logger.info(f"ğŸ“Š Enregistrements AQ aprÃ¨s nettoyage: {aq_clean_count}")

        if aq_clean_count == 0:
            logger.error("âŒ Aucun enregistrement valide aprÃ¨s nettoyage des donnÃ©es AQ")
            raise ValueError("Aucun enregistrement valide aprÃ¨s nettoyage des donnÃ©es AQ")

        # PrÃ©paration des donnÃ©es d'hospitalisation
        logger.info("ğŸ”„ PrÃ©paration des donnÃ©es d'hospitalisation...")

        # VÃ©rification des colonnes nÃ©cessaires
        hosp_columns = df_hosp.columns
        logger.info(f"ğŸ” Colonnes hospitalisation disponibles: {hosp_columns}")

        required_hosp_columns = ['year']
        missing_hosp_columns = [col for col in required_hosp_columns if col not in hosp_columns]

        if missing_hosp_columns:
            logger.error(f"âŒ Colonnes manquantes dans les donnÃ©es hospitalisation: {missing_hosp_columns}")
            raise ValueError(f"Colonnes manquantes dans les donnÃ©es hospitalisation: {missing_hosp_columns}")

        # Nettoyage des donnÃ©es d'hospitalisation
        df_hosp_clean = df_hosp.filter(
            col("year").isNotNull() &
            (col("year") >= 2000) &  # AnnÃ©es rÃ©alistes
            (col("year") <= datetime.date.today().year + 1)
        )

        hosp_clean_count = df_hosp_clean.count()
        logger.info(f"ğŸ“Š Enregistrements hospitalisation aprÃ¨s nettoyage: {hosp_clean_count}")

        if hosp_clean_count == 0:
            logger.error("âŒ Aucun enregistrement valide aprÃ¨s nettoyage des donnÃ©es hospitalisation")
            raise ValueError("Aucun enregistrement valide aprÃ¨s nettoyage des donnÃ©es hospitalisation")

        # VÃ©rification des annÃ©es communes
        years_hosp = df_hosp_clean.select("year").distinct().collect()
        years_hosp_list = [row['year'] for row in years_hosp if row['year'] is not None]
        logger.info(f"ğŸ“… AnnÃ©es prÃ©sentes dans les donnÃ©es hospitalisation: {sorted(years_hosp_list)}")

        common_years = list(set(years_aq_list) & set(years_hosp_list))
        logger.info(f"ğŸ“… AnnÃ©es communes: {sorted(common_years)}")

        if not common_years:
            logger.error("âŒ Aucune annÃ©e commune entre les deux datasets")
            raise ValueError("Aucune annÃ©e commune entre les deux datasets")

        # AgrÃ©gation des donnÃ©es de qualitÃ© de l'air par annÃ©e
        logger.info("ğŸ“Š AgrÃ©gation des donnÃ©es de qualitÃ© de l'air par annÃ©e...")

        df_aq_agg = df_aq_clean.groupBy("year").agg(
            spark_avg("pm25").alias("pm25_avg_yearly"),
            spark_min("pm25").alias("pm25_min_yearly"),
            spark_max("pm25").alias("pm25_max_yearly"),
            spark_count("pm25").alias("pm25_measurements_count"),
            spark_avg("measurements_count").alias("daily_measurements_avg")
        ).withColumn("pm25_avg_yearly", spark_round(col("pm25_avg_yearly"), 2))

        agg_count = df_aq_agg.count()
        logger.info(f"ğŸ“Š AnnÃ©es avec donnÃ©es AQ agrÃ©gÃ©es: {agg_count}")

        logger.info("ğŸ” AperÃ§u des donnÃ©es AQ agrÃ©gÃ©es:")
        df_aq_agg.orderBy("year").show(10)

        # Jointure des datasets
        logger.info("ğŸ”— Jointure des datasets...")

        # Jointure interne sur l'annÃ©e
        df_combined = df_aq_agg.join(df_hosp_clean, on="year", how="inner")

        combined_count = df_combined.count()
        logger.info(f"ğŸ“Š Enregistrements aprÃ¨s jointure: {combined_count}")

        if combined_count == 0:
            logger.error("âŒ Aucun enregistrement aprÃ¨s jointure")
            raise ValueError("Aucun enregistrement aprÃ¨s jointure")

        # SÃ©lection et rÃ©organisation des colonnes finales
        final_columns = [
            "year",
            "pm25_avg_yearly",
            "pm25_min_yearly",
            "pm25_max_yearly",
            "pm25_measurements_count",
            "daily_measurements_avg"
        ]

        # Ajout des colonnes d'hospitalisation disponibles
        hosp_specific_cols = [col for col in df_hosp_clean.columns if col != "year"]
        final_columns.extend(hosp_specific_cols)

        df_final = df_combined.select(*final_columns)

        logger.info("ğŸ” SchÃ©ma des donnÃ©es combinÃ©es finales:")
        df_final.printSchema()

        # Statistiques des donnÃ©es combinÃ©es
        logger.info("ğŸ“ˆ Statistiques des donnÃ©es combinÃ©es:")
        df_final.describe().show()

        # Affichage d'un Ã©chantillon des donnÃ©es finales
        logger.info("ğŸ” Ã‰chantillon des donnÃ©es finales:")
        df_final.orderBy("year").show(10, truncate=False)

        # Calculs de corrÃ©lation si possible
        numeric_columns = [field.name for field in df_final.schema.fields
                           if field.dataType.simpleString() in ['double', 'float', 'int', 'bigint']]

        if len(numeric_columns) >= 2:
            logger.info("ğŸ“Š Analyse de corrÃ©lation:")
            try:
                # Tentative de calcul de corrÃ©lation entre PM2.5 et donnÃ©es hospitalisation
                pm25_col = "pm25_avg_yearly"

                for col_name in numeric_columns:
                    if col_name != pm25_col and col_name != "year":
                        try:
                            correlation = df_final.stat.corr(pm25_col, col_name)
                            logger.info(f"   CorrÃ©lation {pm25_col} vs {col_name}: {correlation:.4f}")
                        except Exception as e:
                            logger.warning(f"   Impossible de calculer la corrÃ©lation pour {col_name}: {e}")
            except Exception as e:
                logger.warning(f"Erreur lors du calcul de corrÃ©lation: {e}")

        # Sauvegarde des donnÃ©es combinÃ©es
        logger.info(f"ğŸ’¾ Sauvegarde en cours vers: {out_path}")

        df_final.coalesce(1).write.mode("overwrite").parquet(str(out_path))

        logger.info(f"âœ… DonnÃ©es combinÃ©es sauvegardÃ©es avec succÃ¨s dans: {out_path}")

        # Validation du fichier de sortie
        if out_path.exists():
            # Calcul de la taille du dossier Parquet
            total_size = sum(p.stat().st_size for p in out_path.rglob('*') if p.is_file())
            logger.info(f"ğŸ“ Taille du fichier final: {total_size} bytes")

            # Test de relecture pour validation
            logger.info("ğŸ” Validation du fichier de sortie...")
            df_test = spark.read.parquet(str(out_path))
            test_count = df_test.count()

            if test_count == combined_count:
                logger.info(f"âœ… Validation rÃ©ussie: {test_count} enregistrements")

                # RÃ©sumÃ© final
                years_range = df_test.agg(
                    spark_min("year").alias("min_year"),
                    spark_max("year").alias("max_year")
                ).collect()[0]

                logger.info(f"ğŸ“… PÃ©riode couverte: {years_range['min_year']} - {years_range['max_year']}")

                # Statistiques finales PM2.5
                pm25_stats = df_test.agg(
                    spark_avg("pm25_avg_yearly").alias("overall_avg"),
                    spark_min("pm25_avg_yearly").alias("overall_min"),
                    spark_max("pm25_avg_yearly").alias("overall_max")
                ).collect()[0]

                logger.info(f"ğŸŒ¬ï¸ PM2.5 moyen sur la pÃ©riode: {pm25_stats['overall_avg']:.2f} Âµg/mÂ³")
                logger.info(
                    f"ğŸŒ¬ï¸ PM2.5 min/max: {pm25_stats['overall_min']:.2f} - {pm25_stats['overall_max']:.2f} Âµg/mÂ³")

            else:
                logger.error(f"âŒ Erreur de validation: attendu {combined_count}, trouvÃ© {test_count}")
                raise ValueError("Erreur lors de la validation du fichier de sortie")
        else:
            logger.error("âŒ Le fichier de sortie n'a pas Ã©tÃ© crÃ©Ã©")
            raise ValueError("Le fichier de sortie n'a pas Ã©tÃ© crÃ©Ã©")

        logger.info("ğŸ‰ Combinaison des datasets terminÃ©e avec succÃ¨s!")

        # MÃ©triques de rÃ©sumÃ©
        logger.info("=" * 60)
        logger.info("ğŸ“Š RÃ‰SUMÃ‰ DE LA COMBINAISON")
        logger.info("=" * 60)
        logger.info(f"ğŸ“… Date de traitement: {today_str}")
        logger.info(f"ğŸ“Š Enregistrements AQ initiaux: {aq_count}")
        logger.info(f"ğŸ“Š Enregistrements hospitalisation initiaux: {hosp_count}")
        logger.info(f"ğŸ“Š Enregistrements finaux combinÃ©s: {combined_count}")
        logger.info(f"ğŸ“… AnnÃ©es traitÃ©es: {len(common_years)}")
        logger.info(f"ğŸ’¾ Fichier de sortie: {out_path}")
        logger.info("=" * 60)

    except Exception as e:
        logger.error(f"âŒ Erreur lors de la combinaison: {type(e).__name__}: {e}")
        raise

    finally:
        # Nettoyage des ressources Spark
        if spark:
            logger.info("ğŸ§¹ Fermeture de la session Spark...")
            spark.stop()
            logger.info("âœ… Session Spark fermÃ©e")


def validate_combined_data(date_str=None):
    """
    Fonction de validation pour vÃ©rifier les donnÃ©es combinÃ©es
    """
    if date_str is None:
        date_str = datetime.date.today().isoformat()

    combined_path = Path(PROJECT_ROOT) / "data/final" / f"combined_{date_str}.parquet"

    logger.info(f"ğŸ” Validation des donnÃ©es combinÃ©es pour {date_str}")
    logger.info(f"ğŸ“‚ Fichier: {combined_path}")

    if not combined_path.exists():
        logger.error(f"âŒ Fichier combinÃ© introuvable: {combined_path}")
        return False

    try:
        spark = SparkSession.builder.appName("ValidateCombinedData").getOrCreate()
        spark.sparkContext.setLogLevel("WARN")

        df = spark.read.parquet(str(combined_path))
        count = df.count()

        logger.info(f"âœ… Fichier valide avec {count} enregistrements")

        # VÃ©rification de l'intÃ©gritÃ© des donnÃ©es
        null_counts = {}
        for column in df.columns:
            null_count = df.filter(col(column).isNull()).count()
            null_counts[column] = null_count
            if null_count > 0:
                logger.warning(f"âš ï¸ Colonne '{column}': {null_count} valeurs nulles")

        # Statistiques rapides
        logger.info("ğŸ“Š Statistiques des donnÃ©es combinÃ©es:")
        df.describe().show()

        # VÃ©rification des annÃ©es
        year_range = df.agg(
            spark_min("year").alias("min_year"),
            spark_max("year").alias("max_year"),
            spark_count("year").alias("year_count")
        ).collect()[0]

        logger.info(
            f"ğŸ“… AnnÃ©es: {year_range['min_year']} - {year_range['max_year']} ({year_range['year_count']} annÃ©es)")

        spark.stop()
        return True

    except Exception as e:
        logger.error(f"âŒ Erreur de validation: {e}")
        return False


if __name__ == "__main__":
    # Test en standalone
    logger.info("ğŸš€ ExÃ©cution en mode test - Combinaison des Datasets")
    logger.info("=" * 70)

    try:
        combine_datasets()
        logger.info("=" * 70)
        validate_combined_data()

    except Exception as e:
        logger.error(f"âŒ Ã‰chec du test: {e}")
        exit(1)